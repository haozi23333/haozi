---
slug: 2019/08/26/hello-world
title: 使用kubeadm搭建 k8s
authors: haozi
tags: [k8s, docker, 虚拟化, 笔记]
date: 2019-08-26 10:32:51
---


# 使用 kubeadm 搭建 k8s_

上次把内网的虚拟化清理掉了, 包括内网的一些服务全都没了, 这篇文章就记录一下 k8s 的搭建过程吧

<div style={{display:"flex"}}>
![](./使用kubeadm搭建k8s/logo.svg)
<span style={{ lineHeight: "100px",textAlign: "center",fontSize: "80px", color: "#326DE6" }}> &amp; </span>
<img src="/2019/08/26/使用kubeadm搭建k8s/kubeadm-logo.png" style={{
marginLeft: "30px", marginTop: "10px", height: "80px"
}}/>
</div>

<!--truncate-->


## 准备

先把服务器准备好, 搭建 k8s 最小化的集群, 一个 master 2 个 node

* 机器

| \\\\    | Master1     | Node1       | Node2       |
|----------|:-------------:|------:|------:|
| CPU  | 2 核心      | 2 核心      | 2 核心      |
| 内存 | 2G          | 4G          | 4G          |
| 磁盘 | 30G 本地SSD | 30G 本地SSD | 30G 本地SSD |

磁盘给的不多, 后面需要数据持久化的时候远程挂在 NFS 存储即可,性能也给的不多, 后期按需求添加

* 系统 (Centos 7)

所有的均关闭了 SELinux, 防火墙

### 关闭 SELinux

在每一个节点上面运行以下命令, 以关闭 selinux

```sh
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```
### 关闭防火墙
```sh
systemctl stop firewalld.service
systemctl disable firewalld.service
```

###  设置路由

```sh
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
```

### 设置转发

```sh
echo 1 > /proc/sys/net/ipv4/ip_forward
```

### 关闭 Swap 分区

```sh
echo "vm.swappiness = 0">> /etc/sysctl.conf
swapoff -a && swapon -a   # 写回所有数据
sysctl -p  #(执行这个使其生效，不用重启)
# 编辑 /etc/fstab 删除  swap 挂载
# 如果还是无效 reboot 一下
```



## 安装

> 下面的所有步骤均按照 [kubernetes 官方教程](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) 操作, 且使用了代理, 未使用相关的国内镜像服务, 请在相关的 yum, docker 处做好 proxy

### 安装 DockerCE

先安装  `yum-config-manager`

```sh
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
```

添加仓库

```sh
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
```
安装 docker-ce
```sh
sudo yum install docker-ce docker-ce-cli containerd.io -y
sudo systemctl enable --now docker
```

**当前版本建议安装 `18.09`的版本, 但是因为我头铁, 所以选最新的**

### 安装kubeadm

在每一个节点上运行以下命令, 添加 kubernetes 的仓库

```sh
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
```

安装 `kubeadm`

```sh
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
```

启动 `kubeadm`

```sh
systemctl enable --now kubelet
```



## 初始化

## Master 节点

运行

```sh
kubeadm init --pod-network-cidr=10.244.0.0/16
```

如果出现了类似这样的提示

```
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
```

参照 [Container runtimes](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) 修复, 过程如下

```sh
## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker
```

然后按照 [Configure cgroup driver used by kubelet on control-plane node](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)进行修改

编辑 `/etc/default/kubelet`文件添加

```sh
KUBELET_EXTRA_ARGS=--cgroup-driver=systemd
```

然后

```sh
systemctl daemon-reload
systemctl restart kubelet
```

亲测有效



如果发现镜像下载不下来

```sh
#编辑
vi /etc/systemd/system/docker.service.d/http-proxy.conf
# 如果提示文件夹不存在 就先创建
mkdir -p /etc/systemd/system/docker.service.d
#添加

[Service]
Environment="HTTP_PROXY=http://proxy-addr:proxy-port" "HTTPS_PROXY=http://proxy-addr:proxy-port" "NO_PROXY=localhost,127.0.0.1"

# 重启
systemctl daemon-reload
systemctl restart docker

# 下载镜像
kubeadm config images pull
```

如果镜像拉取完成, 重新执行 `init` 操作, 完成之后如下

```sh
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.50.160:6443 --token 8tu5cg.u8frzod9ddjm67lf \
    --discovery-token-ca-cert-hash sha256:68383ad315c9da26f8d3848d216477dc109b24787960e6cc7065913476dabca2
```

执行其中的

```
sudo cp /etc/kubernetes/admin.conf $HOME/
sudo chown $(id -u):$(id -g) $HOME/admin.conf
export KUBECONFIG=$HOME/admin.conf
```



记录 Master 机器的这个信息以后要用到

```sh
kubeadm join 192.168.50.160:6443 --token te0pye.vdexzm8onjil28g6 \
    --discovery-token-ca-cert-hash sha256:68383ad315c9da26f8d3848d216477dc109b24787960e6cc7065913476dabca2
```



## Node 节点

在每个 node 节点上面运行上面 master 节点保存的 `kubeadm join`,如果出现了问题, 参考上面 master 的处理方式进行操作

 加入到 Master 的集群里面

> 这里我遇到了一个问题, 我忘记修改 hostname 了, 导致三个节点的 hostname 为同一个, 虽然下 join 的时候不会报错, 但是get nodes 的时候除了 master 其他的都不会显示, 修改方式为 ` hostnamectl  set-hostname <hostname>`

上面的所有操作完成之后 `kubectl get nodes` 如下图
```sh
[root@localhost ~]# kubectl get node
NAME               STATUS     ROLES    AGE     VERSION
haozi.k8s-master   NotReady   master   4m13s   v1.15.3
haozi.k8s-node1    NotReady   <none>   3m12s   v1.15.3
haozi.k8s-node2    NotReady   <none>   67s     v1.15.3
```

不过这个时候所有的节点都处在 `NotReady`的状态, 这时候还缺一个 网络插件, 这里我们安装`flannel`

在 Master 上面执行

```sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/62e44c867a2846fefb68bd5f178daf4da3095ccb/Documentation/kube-flannel.yml
```

等待完成之后 运行 `kubectl get pods -A`

```
[root@localhost ~]# kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-5c98db65d4-85xx4                   1/1     Running   0          2m53s
kube-system   coredns-5c98db65d4-lp8w9                   1/1     Running   0          2m54s
kube-system   etcd-haozi.k8s-master                      1/1     Running   0          114s
kube-system   kube-apiserver-haozi.k8s-master            1/1     Running   0          2m1s
kube-system   kube-controller-manager-haozi.k8s-master   1/1     Running   0          113s
kube-system   kube-flannel-ds-amd64-65wmg                1/1     Running   0          71s
kube-system   kube-flannel-ds-amd64-q5jpt                1/1     Running   0          71s
kube-system   kube-flannel-ds-amd64-rm5mb                1/1     Running   0          71s
kube-system   kube-proxy-26bfq                           1/1     Running   0          2m24s
kube-system   kube-proxy-qmb7p                           1/1     Running   0          2m54s
kube-system   kube-proxy-zsmpv                           1/1     Running   0          2m27s
kube-system   kube-scheduler-haozi.k8s-master            1/1     Running   0          107s
```

可以看到 flannel 的组建都已经 Running, 这时候我们再查看一下 node 的状态

```
[root@localhost ~]# kubectl get node
NAME               STATUS   ROLES    AGE     VERSION
haozi.k8s-master   Ready    master   4m16s   v1.15.3
haozi.k8s-node1    Ready    <none>   3m31s   v1.15.3
haozi.k8s-node2    Ready    <none>   3m28s   v1.15.3
```

都已经 Ready,

致此, 基本安装已经结束

